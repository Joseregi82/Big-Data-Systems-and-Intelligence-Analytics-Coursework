{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received command c on object id p1\n",
      "Received command c on object id p1\n",
      "Received command c on object id p2\n",
      "Received command c on object id p2\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import random\n",
    "from datetime import datetime\n",
    "random.seed(datetime.now())\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Make plots larger\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received command c on object id p1\n",
      "Received command c on object id p1\n",
      "Received command c on object id p2\n",
      "Received command c on object id p2\n",
      "Received command c on object id p1\n",
      "Received command c on object id p1\n",
      "Received command c on object id p2\n",
      "Received command c on object id p2\n",
      "Received command c on object id p1\n",
      "Received command c on object id p1\n",
      "Received command c on object id p2\n",
      "Received command c on object id p2\n",
      "Received command c on object id p1\n",
      "Received command c on object id p1\n",
      "Received command c on object id p2\n",
      "Received command c on object id p2\n"
     ]
    }
   ],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"My App\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.87:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>My App</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=My App>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PACKAGE_EXTENSIONS',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_accumulatorServer',\n",
       " '_active_spark_context',\n",
       " '_batchSize',\n",
       " '_callsite',\n",
       " '_checkpointFile',\n",
       " '_conf',\n",
       " '_dictToJavaMap',\n",
       " '_do_init',\n",
       " '_ensure_initialized',\n",
       " '_gateway',\n",
       " '_getJavaStorageLevel',\n",
       " '_initialize_context',\n",
       " '_javaAccumulator',\n",
       " '_jsc',\n",
       " '_jvm',\n",
       " '_lock',\n",
       " '_next_accum_id',\n",
       " '_pickled_broadcast_vars',\n",
       " '_python_includes',\n",
       " '_repr_html_',\n",
       " '_temp_dir',\n",
       " '_unbatched_serializer',\n",
       " 'accumulator',\n",
       " 'addFile',\n",
       " 'addPyFile',\n",
       " 'appName',\n",
       " 'applicationId',\n",
       " 'binaryFiles',\n",
       " 'binaryRecords',\n",
       " 'broadcast',\n",
       " 'cancelAllJobs',\n",
       " 'cancelJobGroup',\n",
       " 'defaultMinPartitions',\n",
       " 'defaultParallelism',\n",
       " 'dump_profiles',\n",
       " 'emptyRDD',\n",
       " 'environment',\n",
       " 'getConf',\n",
       " 'getLocalProperty',\n",
       " 'getOrCreate',\n",
       " 'hadoopFile',\n",
       " 'hadoopRDD',\n",
       " 'master',\n",
       " 'newAPIHadoopFile',\n",
       " 'newAPIHadoopRDD',\n",
       " 'parallelize',\n",
       " 'pickleFile',\n",
       " 'profiler_collector',\n",
       " 'pythonExec',\n",
       " 'pythonVer',\n",
       " 'range',\n",
       " 'runJob',\n",
       " 'sequenceFile',\n",
       " 'serializer',\n",
       " 'setCheckpointDir',\n",
       " 'setJobGroup',\n",
       " 'setLocalProperty',\n",
       " 'setLogLevel',\n",
       " 'setSystemProperty',\n",
       " 'show_profiles',\n",
       " 'sparkHome',\n",
       " 'sparkUser',\n",
       " 'startTime',\n",
       " 'statusTracker',\n",
       " 'stop',\n",
       " 'textFile',\n",
       " 'uiWebUrl',\n",
       " 'union',\n",
       " 'version',\n",
       " 'wholeTextFiles']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "...     .master(\"local\") \\\n",
    "...     .appName(\"My App\") \\\n",
    "...     .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "...     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.87:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>My App</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=My App>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+--------+-------+--------+-------+-----------+---+----+----+----+----+----------+-----------+-------------+-----------+-------------+---------+--------------------+----------+-----------------+---------------------+-----------------+---------------------+--------------------+------+--------------------+-----+--------------------+--------------------+-----------+--------+\n",
      "|AudioFormat|BitsPerSample|BlockAlign|ByteRate|ChunkID|DataSize| Format|NumChannels| S1|  S2|  S3|  S4|  S5|SampleRate|SubChunk1ID|SubChunk1Size|SubChunk2ID|SubChunk2Size|TotalSize|            filename|instrument|instrument_family|instrument_family_str|instrument_source|instrument_source_str|      instrument_str|  note|            note_str|pitch|           qualities|       qualities_str|sample_rate|velocity|\n",
      "+-----------+-------------+----------+--------+-------+--------+-------+-----------+---+----+----+----+----+----------+-----------+-------------+-----------+-------------+---------+--------------------+----------+-----------------+---------------------+-----------------+---------------------+--------------------+------+--------------------+-----+--------------------+--------------------+-----------+--------+\n",
      "|          1|           16|         2|   32000|b'RIFF'|  128000|b'WAVE'|          1|  0|   0|   0|   0|   0|     16000|    b'fmt '|           16|    b'data'|       128000|   128044|keyboard_syntheti...|       577|                4|             keyboard|                2|            synthetic|keyboard_syntheti...|297525|keyboard_syntheti...|  100|[1, 0, 1, 1, 0, 0...|['bright', 'disto...|      16000|     100|\n",
      "|          1|           16|         2|   32000|b'RIFF'|  128000|b'WAVE'|          1|  0|   0|   0|   0|   0|     16000|    b'fmt '|           16|    b'data'|       128000|   128044|keyboard_syntheti...|       577|                4|             keyboard|                2|            synthetic|keyboard_syntheti...|159977|keyboard_syntheti...|  101|[1, 0, 1, 1, 0, 0...|['bright', 'disto...|      16000|     100|\n",
      "|          1|           16|         2|   32000|b'RIFF'|  128000|b'WAVE'|          1|  0|   0|   0|   0|   0|     16000|    b'fmt '|           16|    b'data'|       128000|   128044|keyboard_syntheti...|       577|                4|             keyboard|                2|            synthetic|keyboard_syntheti...|294611|keyboard_syntheti...|  103|[1, 0, 1, 1, 0, 0...|['bright', 'disto...|      16000|     127|\n",
      "|          1|           16|         2|   32000|b'RIFF'|  128000|b'WAVE'|          1|  0|   0|   0|   0|   0|     16000|    b'fmt '|           16|    b'data'|       128000|   128044|keyboard_syntheti...|       577|                4|             keyboard|                2|            synthetic|keyboard_syntheti...|247095|keyboard_syntheti...|  104|[1, 0, 1, 1, 0, 0...|['bright', 'disto...|      16000|     127|\n",
      "|          1|           16|         2|   32000|b'RIFF'|  128000|b'WAVE'|          1|  0|   0|   0|   0|   0|     16000|    b'fmt '|           16|    b'data'|       128000|   128044|keyboard_syntheti...|       577|                4|             keyboard|                2|            synthetic|keyboard_syntheti...|180685|keyboard_syntheti...|   93|[1, 0, 1, 1, 0, 0...|['bright', 'disto...|      16000|     127|\n",
      "|          1|           16|         2|   32000|b'RIFF'|  128000|b'WAVE'|          1|  0|   0|   0|   0|   0|     16000|    b'fmt '|           16|    b'data'|       128000|   128044|keyboard_syntheti...|       577|                4|             keyboard|                2|            synthetic|keyboard_syntheti...|243490|keyboard_syntheti...|   94|[1, 0, 1, 1, 0, 0...|['bright', 'disto...|      16000|     100|\n",
      "|          1|           16|         2|   32000|b'RIFF'|  128000|b'WAVE'|          1|  0|   0|   0|   0|   0|     16000|    b'fmt '|           16|    b'data'|       128000|   128044|keyboard_syntheti...|       577|                4|             keyboard|                2|            synthetic|keyboard_syntheti...|230398|keyboard_syntheti...|   95|[1, 0, 1, 1, 0, 0...|['bright', 'disto...|      16000|     127|\n",
      "|          1|           16|         2|   32000|b'RIFF'|  128000|b'WAVE'|          1|  0|   0|   0|   0|   0|     16000|    b'fmt '|           16|    b'data'|       128000|   128044|keyboard_syntheti...|       577|                4|             keyboard|                2|            synthetic|keyboard_syntheti...|296293|keyboard_syntheti...|  100|[1, 0, 1, 1, 0, 0...|['bright', 'disto...|      16000|      75|\n",
      "|          1|           16|         2|   32000|b'RIFF'|  128000|b'WAVE'|          1|  0|   0|   0|   0|   0|     16000|    b'fmt '|           16|    b'data'|       128000|   128044|keyboard_syntheti...|       577|                4|             keyboard|                2|            synthetic|keyboard_syntheti...|201933|keyboard_syntheti...|  102|[1, 0, 1, 1, 0, 0...|['bright', 'disto...|      16000|      25|\n",
      "|          1|           16|         2|   32000|b'RIFF'|  128000|b'WAVE'|          1|  0|   0|   0|   0|   0|     16000|    b'fmt '|           16|    b'data'|       128000|   128044|keyboard_syntheti...|       577|                4|             keyboard|                2|            synthetic|keyboard_syntheti...|200029|keyboard_syntheti...|  107|[1, 0, 1, 1, 0, 0...|['bright', 'disto...|      16000|      75|\n",
      "|          1|           16|         2|   32000|b'RIFF'|  128000|b'WAVE'|          1|  0|   0|   0|   0|   0|     16000|    b'fmt '|           16|    b'data'|       128000|   128044|keyboard_syntheti...|       577|                4|             keyboard|                2|            synthetic|keyboard_syntheti...|293592|keyboard_syntheti...|  108|[1, 0, 1, 1, 0, 0...|['bright', 'disto...|      16000|      50|\n",
      "|          1|           16|         2|   32000|b'RIFF'|  128000|b'WAVE'|          1|  0|   0|   0|   0|   0|     16000|    b'fmt '|           16|    b'data'|       128000|   128044|keyboard_syntheti...|       577|                4|             keyboard|                2|            synthetic|keyboard_syntheti...|181817|keyboard_syntheti...|   92|[1, 0, 1, 1, 0, 0...|['bright', 'disto...|      16000|      50|\n",
      "|          1|           16|         2|   32000|b'RIFF'|  128000|b'WAVE'|          1|  0|   0|   0|   0|   0|     16000|    b'fmt '|           16|    b'data'|       128000|   128044|keyboard_syntheti...|       577|                4|             keyboard|                2|            synthetic|keyboard_syntheti...|264225|keyboard_syntheti...|   93|[1, 0, 1, 1, 0, 0...|['bright', 'disto...|      16000|      75|\n",
      "|          1|           16|         2|   32000|b'RIFF'|  128000|b'WAVE'|          1|  0|   0|   0|   0|   0|     16000|    b'fmt '|           16|    b'data'|       128000|   128044|keyboard_syntheti...|       577|                4|             keyboard|                2|            synthetic|keyboard_syntheti...|248017|keyboard_syntheti...|   98|[1, 0, 1, 1, 0, 0...|['bright', 'disto...|      16000|      75|\n",
      "|          1|           16|         2|   32000|b'RIFF'|  128000|b'WAVE'|          1|  0|   0|   0|   0|   0|     16000|    b'fmt '|           16|    b'data'|       128000|   128044|keyboard_syntheti...|       577|                4|             keyboard|                2|            synthetic|keyboard_syntheti...|204079|keyboard_syntheti...|   94|[0, 0, 1, 1, 0, 0...|['distortion', 'f...|      16000|      25|\n",
      "|          1|           16|         2|   32000|b'RIFF'|  128000|b'WAVE'|          1| 11|-205|-335|-355|-484|     16000|    b'fmt '|           16|    b'data'|       128000|   128044|vocal_synthetic_0...|        37|               10|                vocal|                2|            synthetic| vocal_synthetic_003| 58209|vocal_synthetic_0...|   30|[1, 0, 1, 0, 1, 0...|['bright', 'disto...|      16000|     127|\n",
      "|          1|           16|         2|   32000|b'RIFF'|  128000|b'WAVE'|          1| -8|-226|-356|-434|-363|     16000|    b'fmt '|           16|    b'data'|       128000|   128044|vocal_synthetic_0...|        37|               10|                vocal|                2|            synthetic| vocal_synthetic_003| 16138|vocal_synthetic_0...|   39|[1, 0, 1, 0, 1, 0...|['bright', 'disto...|      16000|     127|\n",
      "|          1|           16|         2|   32000|b'RIFF'|  128000|b'WAVE'|          1| -1|-252|-367|-466|-242|     16000|    b'fmt '|           16|    b'data'|       128000|   128044|vocal_synthetic_0...|        37|               10|                vocal|                2|            synthetic| vocal_synthetic_003| 25540|vocal_synthetic_0...|   40|[1, 0, 1, 0, 1, 0...|['bright', 'disto...|      16000|     127|\n",
      "|          1|           16|         2|   32000|b'RIFF'|  128000|b'WAVE'|          1|-16|-243|-392|-193| 410|     16000|    b'fmt '|           16|    b'data'|       128000|   128044|vocal_synthetic_0...|        37|               10|                vocal|                2|            synthetic| vocal_synthetic_003| 49106|vocal_synthetic_0...|   44|[1, 0, 1, 0, 1, 0...|['bright', 'disto...|      16000|     127|\n",
      "|          1|           16|         2|   32000|b'RIFF'|  128000|b'WAVE'|          1|-10|-118| 257|-309|-203|     16000|    b'fmt '|           16|    b'data'|       128000|   128044|vocal_synthetic_0...|        37|               10|                vocal|                2|            synthetic| vocal_synthetic_003| 50976|vocal_synthetic_0...|   58|[1, 0, 1, 0, 1, 0...|['bright', 'disto...|      16000|     100|\n",
      "+-----------+-------------+----------+--------+-------+--------+-------+-----------+---+----+----+----+----+----------+-----------+-------------+-----------+-------------+---------+--------------------+----------+-----------------+---------------------+-----------------+---------------------+--------------------+------+--------------------+-----+--------------------+--------------------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "audio = spark.read.json(\"test_data/json_test/*.json\")\n",
    "# Displays the content of the DataFrame to stdout\n",
    "audio.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- AudioFormat: string (nullable = true)\n",
      " |-- BitsPerSample: string (nullable = true)\n",
      " |-- BlockAlign: string (nullable = true)\n",
      " |-- ByteRate: string (nullable = true)\n",
      " |-- ChunkID: string (nullable = true)\n",
      " |-- DataSize: string (nullable = true)\n",
      " |-- Format: string (nullable = true)\n",
      " |-- NumChannels: string (nullable = true)\n",
      " |-- S1: string (nullable = true)\n",
      " |-- S2: string (nullable = true)\n",
      " |-- S3: string (nullable = true)\n",
      " |-- S4: string (nullable = true)\n",
      " |-- S5: string (nullable = true)\n",
      " |-- SampleRate: string (nullable = true)\n",
      " |-- SubChunk1ID: string (nullable = true)\n",
      " |-- SubChunk1Size: string (nullable = true)\n",
      " |-- SubChunk2ID: string (nullable = true)\n",
      " |-- SubChunk2Size: string (nullable = true)\n",
      " |-- TotalSize: string (nullable = true)\n",
      " |-- filename: string (nullable = true)\n",
      " |-- instrument: string (nullable = true)\n",
      " |-- instrument_family: string (nullable = true)\n",
      " |-- instrument_family_str: string (nullable = true)\n",
      " |-- instrument_source: string (nullable = true)\n",
      " |-- instrument_source_str: string (nullable = true)\n",
      " |-- instrument_str: string (nullable = true)\n",
      " |-- note: string (nullable = true)\n",
      " |-- note_str: string (nullable = true)\n",
      " |-- pitch: string (nullable = true)\n",
      " |-- qualities: string (nullable = true)\n",
      " |-- qualities_str: string (nullable = true)\n",
      " |-- sample_rate: string (nullable = true)\n",
      " |-- velocity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "audio.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AudioFormat',\n",
       " 'BitsPerSample',\n",
       " 'BlockAlign',\n",
       " 'ByteRate',\n",
       " 'ChunkID',\n",
       " 'DataSize',\n",
       " 'Format',\n",
       " 'NumChannels',\n",
       " 'S1',\n",
       " 'S2',\n",
       " 'S3',\n",
       " 'S4',\n",
       " 'S5',\n",
       " 'SampleRate',\n",
       " 'SubChunk1ID',\n",
       " 'SubChunk1Size',\n",
       " 'SubChunk2ID',\n",
       " 'SubChunk2Size',\n",
       " 'TotalSize',\n",
       " 'filename',\n",
       " 'instrument',\n",
       " 'instrument_family',\n",
       " 'instrument_family_str',\n",
       " 'instrument_source',\n",
       " 'instrument_source_str',\n",
       " 'instrument_str',\n",
       " 'note',\n",
       " 'note_str',\n",
       " 'pitch',\n",
       " 'qualities',\n",
       " 'qualities_str',\n",
       " 'sample_rate',\n",
       " 'velocity']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, AudioFormat: string, BitsPerSample: string, BlockAlign: string, ByteRate: string, ChunkID: string, DataSize: string, Format: string, NumChannels: string, S1: string, S2: string, S3: string, S4: string, S5: string, SampleRate: string, SubChunk1ID: string, SubChunk1Size: string, SubChunk2ID: string, SubChunk2Size: string, TotalSize: string, filename: string, instrument: string, instrument_family: string, instrument_family_str: string, instrument_source: string, instrument_source_str: string, instrument_str: string, note: string, note_str: string, pitch: string, qualities: string, qualities_str: string, sample_rate: string, velocity: string]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------+----------+--------+-------+--------+-------+-----------+------------------+-----------------+------------------+-----------------+-----------------+----------+-----------+-------------+-----------+-------------+---------+--------------------+-----------------+------------------+---------------------+------------------+---------------------+-------------------+------------------+--------------------+------------------+--------------------+--------------------+-----------+-----------------+\n",
      "|summary|AudioFormat|BitsPerSample|BlockAlign|ByteRate|ChunkID|DataSize| Format|NumChannels|                S1|               S2|                S3|               S4|               S5|SampleRate|SubChunk1ID|SubChunk1Size|SubChunk2ID|SubChunk2Size|TotalSize|            filename|       instrument| instrument_family|instrument_family_str| instrument_source|instrument_source_str|     instrument_str|              note|            note_str|             pitch|           qualities|       qualities_str|sample_rate|         velocity|\n",
      "+-------+-----------+-------------+----------+--------+-------+--------+-------+-----------+------------------+-----------------+------------------+-----------------+-----------------+----------+-----------+-------------+-----------+-------------+---------+--------------------+-----------------+------------------+---------------------+------------------+---------------------+-------------------+------------------+--------------------+------------------+--------------------+--------------------+-----------+-----------------+\n",
      "|  count|       4096|         4096|      4096|    4096|   4096|    4096|   4096|       4096|              4096|             4096|              4096|             4096|             4096|      4096|       4096|         4096|       4096|         4096|     4096|                4096|             4096|              4096|                 4096|              4096|                 4096|               4096|              4096|                4096|              4096|                4096|                4096|       4096|             4096|\n",
      "|   mean|        1.0|         16.0|       2.0| 32000.0|   null|128000.0|   null|        1.0|     4.31884765625|      376.8984375|   461.88427734375|  199.99755859375|   250.3583984375|   16000.0|       null|         16.0|       null|     128000.0| 128044.0|                null|   423.4970703125|     3.70458984375|                 null|     0.84033203125|                 null|               null|149040.89575195312|                null|   61.480712890625|                null|                null|    16000.0|   75.28857421875|\n",
      "| stddev|        0.0|          0.0|       0.0|     0.0|   null|     0.0|   null|        0.0|246.95722001779174|2214.614629667316|2870.2999950057956|2811.107774550908|3127.064203109915|       0.0|       null|          0.0|       null|          0.0|      0.0|                null|300.1980664699458|2.7611712805035795|                 null|0.7998133403174874|                 null|               null| 90014.14699681636|                null|23.386503920969226|                null|                null|        0.0|35.77511807565207|\n",
      "|    min|          1|           16|         2|   32000|b'RIFF'|  128000|b'WAVE'|          1|                -1|               -1|                -1|               -1|               -1|     16000|    b'fmt '|           16|    b'data'|       128000|   128044|bass_electronic_0...|              100|                 0|                 bass|                 0|             acoustic|bass_electronic_018|            100037|bass_electronic_0...|                10|[0, 0, 0, 0, 0, 0...|['bright', 'disto...|      16000|              100|\n",
      "|    max|          1|           16|         2|   32000|b'RIFF'|  128000|b'WAVE'|          1|               995|               99|               997|             9956|             9938|     16000|    b'fmt '|           16|    b'data'|       128000|   128044|vocal_synthetic_0...|               97|                 8|                vocal|                 2|            synthetic|vocal_synthetic_003|             99961|vocal_synthetic_0...|                99|[1, 0, 1, 1, 0, 0...|                  []|      16000|               75|\n",
      "+-------+-----------+-------------+----------+--------+-------+--------+-------+-----------+------------------+-----------------+------------------+-----------------+-----------------+----------+-----------+-------------+-----------+-------------+---------+--------------------+-----------------+------------------+---------------------+------------------+---------------------+-------------------+------------------+--------------------+------------------+--------------------+--------------------+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "audio.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'AudioFormat'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio['AudioFormat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|ByteRate|velocity|\n",
      "+--------+--------+\n",
      "|   32000|     100|\n",
      "|   32000|     100|\n",
      "|   32000|     127|\n",
      "|   32000|     127|\n",
      "|   32000|     127|\n",
      "|   32000|     100|\n",
      "|   32000|     127|\n",
      "|   32000|      75|\n",
      "|   32000|      25|\n",
      "|   32000|      75|\n",
      "|   32000|      50|\n",
      "|   32000|      50|\n",
      "|   32000|      75|\n",
      "|   32000|      75|\n",
      "|   32000|      25|\n",
      "|   32000|     127|\n",
      "|   32000|     127|\n",
      "|   32000|     127|\n",
      "|   32000|     127|\n",
      "|   32000|     100|\n",
      "+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "audio.select(['ByteRate','velocity']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.87:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>My App</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10f9ffeb8>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"My App\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame representing the stream of input lines from connection to localhost:9999\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Split the lines into words\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = lines.select(\n",
    "   explode(\n",
    "       split(lines.value,\"\")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "# Generate running word count\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string, count: bigint]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts = words.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start running the query that prints the running counts to the console\n",
    "query = wordCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.regression import StreamingLinearRegressionWithSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 1)\n",
    "def parse(lp):\n",
    "    label = float(lp[lp.find('(') + 1: lp.find(',')])\n",
    "    vec = Vectors.dense(lp[lp.find('[') + 1: lp.find(']')].split(','))\n",
    "    return LabeledPoint(label, vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingData = ssc.textFileStream(sys.argv[1]).map(parse).cache()\n",
    "testData = ssc.textFileStream(sys.argv[2]).map(parse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.streaming.dstream.TransformedDStream object at 0x10fc7be80>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-d3e5d9d7289d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/shrutimehta/anaconda/lib/python3.5/site-packages/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \"\"\"\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shrutimehta/anaconda/lib/python3.5/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/Users/shrutimehta/anaconda/lib/python3.5/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shrutimehta/anaconda/lib/python3.5/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shrutimehta/anaconda/lib/python3.5/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "numFeatures = 3\n",
    "model = StreamingLinearRegressionWithSGD()\n",
    "model.setInitialWeights([0.0, 0.0, 0.0])\n",
    "\n",
    "model.trainOn(trainingData)\n",
    "print(model.predictOnValues(testData.map(lambda lp: (lp.label, lp.features))))\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logistics regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Load and parse the data\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(values[0], values[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1770] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.textFile(\"test_data/json_test/1.json\")\n",
    "parsedData = data.map(parsePoint)\n",
    "parsedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 35.0 failed 1 times, most recent failure: Lost task 0.0 in stage 35.0 (TID 33426, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/shrutimehta/anaconda/lib/python3.5/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/Users/shrutimehta/anaconda/lib/python3.5/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/shrutimehta/anaconda/lib/python3.5/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/shrutimehta/anaconda/lib/python3.5/site-packages/pyspark/rdd.py\", line 1339, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-93-80871a8abe9f>\", line 6, in parsePoint\n  File \"<ipython-input-93-80871a8abe9f>\", line 6, in <listcomp>\nValueError: could not convert string to float: '{\"filename\":'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/shrutimehta/anaconda/lib/python3.5/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/Users/shrutimehta/anaconda/lib/python3.5/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/shrutimehta/anaconda/lib/python3.5/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/shrutimehta/anaconda/lib/python3.5/site-packages/pyspark/rdd.py\", line 1339, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-93-80871a8abe9f>\", line 6, in parsePoint\n  File \"<ipython-input-93-80871a8abe9f>\", line 6, in <listcomp>\nValueError: could not convert string to float: '{\"filename\":'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-1a5c14d3edb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegressionWithLBFGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsedData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/shrutimehta/anaconda/lib/python3.5/site-packages/pyspark/mllib/classification.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, data, iterations, initialWeights, regParam, regType, intercept, corrections, tolerance, validateData, numClasses)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitialWeights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnumClasses\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m                 \u001b[0minitialWeights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mintercept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shrutimehta/anaconda/lib/python3.5/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \"\"\"\n\u001b[0;32m-> 1361\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shrutimehta/anaconda/lib/python3.5/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shrutimehta/anaconda/lib/python3.5/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shrutimehta/anaconda/lib/python3.5/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shrutimehta/anaconda/lib/python3.5/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shrutimehta/anaconda/lib/python3.5/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 35.0 failed 1 times, most recent failure: Lost task 0.0 in stage 35.0 (TID 33426, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/shrutimehta/anaconda/lib/python3.5/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/Users/shrutimehta/anaconda/lib/python3.5/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/shrutimehta/anaconda/lib/python3.5/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/shrutimehta/anaconda/lib/python3.5/site-packages/pyspark/rdd.py\", line 1339, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-93-80871a8abe9f>\", line 6, in parsePoint\n  File \"<ipython-input-93-80871a8abe9f>\", line 6, in <listcomp>\nValueError: could not convert string to float: '{\"filename\":'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:446)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/shrutimehta/anaconda/lib/python3.5/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/Users/shrutimehta/anaconda/lib/python3.5/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/shrutimehta/anaconda/lib/python3.5/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/shrutimehta/anaconda/lib/python3.5/site-packages/pyspark/rdd.py\", line 1339, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-93-80871a8abe9f>\", line 6, in parsePoint\n  File \"<ipython-input-93-80871a8abe9f>\", line 6, in <listcomp>\nValueError: could not convert string to float: '{\"filename\":'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = LogisticRegressionWithLBFGS.train(parsedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluating the model on training data\n",
    "labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda lp: lp[0] != lp[1]).count() / float(parsedData.count())\n",
    "print(\"Training Error = \" + str(trainErr))\n",
    "\n",
    "# Save and load model\n",
    "model.save(sc, \"target/tmp/pythonLogisticRegressionWithLBFGSModel\")\n",
    "sameModel = LogisticRegressionModel.load(sc,\"target/tmp/pythonLogisticRegressionWithLBFGSModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import NaiveBayes\n",
    "\n",
    "# an RDD of LabeledPoint\n",
    "data = sc.parallelize([\n",
    "  LabeledPoint(0.0, [0.0, 0.0])\n",
    "# more labeled points\n",
    "])\n",
    "\n",
    "# Train a naive Bayes model.\n",
    "model = NaiveBayes.train(data, 1.0)\n",
    "\n",
    "# Make prediction.\n",
    "prediction = model.predict([0.0, 0.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "categoricalColumns = [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\"]\n",
    "stages = [] # stages in our Pipeline\n",
    "for categoricalCol in categoricalColumns:\n",
    "# Category Indexing with StringIndexer\n",
    "  stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n",
    "# Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "  encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n",
    "# Add stages.  These are not run here, but will run all at once later on.\n",
    "  stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pixiedust'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-d09babb4fd2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpixiedust\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackageManager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPackageManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpkg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPackageManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpkg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstallPackage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"graphframes:graphframes:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpkg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintAllPackages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pixiedust'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received command c on object id p1\n",
      "Received command c on object id p1\n",
      "Received command c on object id p2\n",
      "Received command c on object id p2\n",
      "Received command c on object id p1\n",
      "Received command c on object id p1\n",
      "Received command c on object id p2\n",
      "Received command c on object id p2\n",
      "Received command c on object id p1\n",
      "Received command c on object id p1\n",
      "Received command c on object id p2\n",
      "Received command c on object id p2\n",
      "Received command c on object id p1\n",
      "Received command c on object id p1\n",
      "Received command c on object id p2\n",
      "Received command c on object id p2\n",
      "Received command c on object id p1\n",
      "Received command c on object id p1\n",
      "Received command c on object id p2\n",
      "Received command c on object id p2\n",
      "Received command c on object id p1\n",
      "Received command c on object id p1\n",
      "Received command c on object id p2\n",
      "Received command c on object id p2\n",
      "Received command c on object id p1\n",
      "Received command c on object id p1\n",
      "Received command c on object id p2\n",
      "Received command c on object id p2\n",
      "Received command c on object id p1\n",
      "Received command c on object id p1\n",
      "Received command c on object id p2\n",
      "Received command c on object id p2\n",
      "Received command c on object id p1\n",
      "Received command c on object id p1\n",
      "Received command c on object id p2\n",
      "Received command c on object id p2\n",
      "Received command c on object id p1\n",
      "Received command c on object id p1\n",
      "Received command c on object id p2\n",
      "Received command c on object id p2\n",
      "Received command c on object id p1\n",
      "Received command c on object id p1\n",
      "Received command c on object id p2\n",
      "Received command c on object id p2\n",
      "Received command c on object id p1\n",
      "Received command c on object id p1\n",
      "Received command c on object id p2\n",
      "Received command c on object id p2\n"
     ]
    }
   ],
   "source": [
    "from pixiedust.packageManager import PackageManager\n",
    "pkg=PackageManager()\n",
    "pkg.installPackage(\"graphframes:graphframes:0\")\n",
    "pkg.printAllPackages()\n",
    " \n",
    "sqlContext=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
